<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on Hugo ʕ•ᴥ•ʔ Bear Blog</title>
    <link>https://zhiwei-lii.github.io/blog/</link>
    <description>Recent content in Blog on Hugo ʕ•ᴥ•ʔ Bear Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <copyright>Copyright © 2020, Jane Doe.</copyright>
    <lastBuildDate>Tue, 28 May 2024 10:17:36 +0800</lastBuildDate><atom:link href="https://zhiwei-lii.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>关于intel_pstate下面的powersave和performance governor</title>
      <link>https://zhiwei-lii.github.io/%E5%85%B3%E4%BA%8Eintel_pstate%E4%B8%8B%E9%9D%A2%E7%9A%84powersave%E5%92%8Cperformance-governor/</link>
      <pubDate>Tue, 28 May 2024 10:17:36 +0800</pubDate>
      
      <guid>https://zhiwei-lii.github.io/%E5%85%B3%E4%BA%8Eintel_pstate%E4%B8%8B%E9%9D%A2%E7%9A%84powersave%E5%92%8Cperformance-governor/</guid>
      <description>我们都知道, powersave和performance是cpufreq里面两个不同的governor, 用于控制cpu进入不同的频率(p_state).
但是我一直有这样一个固有认知:
 当cpu处于powersave mode时, 出于省电考虑, cpufreq会运行在最低频率. 当cpu处于performance mode时, 出于性能考虑, cpufreq会运行在turbo允许的最高频率.  但是最近才发现: 当我们的cpufreq driver = intel_pstate时, powersave其实更像是以前的ondemand/schedutil, 即根据实际情况, 动态调整power和performance之间的关系, 而不是死板地运行在最低频率.
zhiwei@zhiwei-pc:~$ cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_driver intel_pstate zhiwei@zhiwei-pc:~$ zhiwei@zhiwei-pc:~$ cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor powersave # 在intel_pstate下面, 只有performance和powersave两种governor zhiwei@zhiwei-pc:~$ cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_available_governors performance powersave 然后我就去查看了一下相关的kernel code, 试图理解: 当我们把governor设成powersave之后, 发生了什么.
// 这是kernel 6.1, 调用链大致如下: // store_scaling_governor -&amp;gt; cpufreq_set_policy -&amp;gt; intel_pstate_set_policy -&amp;gt; intel_pstate_hwp_set // drivers/cpufreq/intel_pstate.c static void intel_pstate_hwp_set(unsigned int cpu) { ... // 这里的max/min对应了scaling_max_freq/scaling_min_freq max = cpu_data-&amp;gt;max_perf_ratio; min = cpu_data-&amp;gt;min_perf_ratio; // 如果是performance的话, 把min也设成max, 达到性能最大化 if (cpu_data-&amp;gt;policy == CPUFREQ_POLICY_PERFORMANCE) min = max; // 读取MSR_HWP_REQUEST这个寄存器的值 rdmsrl_on_cpu(cpu, MSR_HWP_REQUEST, &amp;amp;value); value &amp;amp;= ~HWP_MIN_PERF(~0L); value |= HWP_MIN_PERF(min); value &amp;amp;= ~HWP_MAX_PERF(~0L); value |= HWP_MAX_PERF(max); .</description>
    </item>
    
    <item>
      <title>Gfxreconstruct replay Linux/Windows game trace</title>
      <link>https://zhiwei-lii.github.io/gfxreconstruct-replay-linux/windows-game-trace/</link>
      <pubDate>Mon, 17 Apr 2023 13:50:36 +0800</pubDate>
      
      <guid>https://zhiwei-lii.github.io/gfxreconstruct-replay-linux/windows-game-trace/</guid>
      <description>最近在研究vkd3d的性能, 在开始研究之前, 有一个前置任务: 用gfxreconstruct去replay game trace, 同时要确保这个trace既能在windows上面运行, 又能在linux上面运行. 这样可以方便我们比较linux和windows的差异. 本文记录一下, 在enable replay trace的时候, 遇到的一些问题, 以及解决方案.
第一个尝试是: 在windows上面用vkd3d运行dx12 game, 然后把trace抓下来, 再拿到linux上面去replay. 这样的做法是保证: 游戏看到的vulkan extension在不同OS上是基本一致的(因为都要经过vkd3d)
但是当我试图在linux上面replay这个trace的时候, 果不其然失败了(事情总是没有那么容易). 遇到了以下错误.
 1 [gfxrecon] WARNING - Mismatch: 2 [gfxrecon] WARNING - Captured application name: Cyberpunk2077.exe 3 [gfxrecon] WARNING - Replayer process name: gfxrecon-replay.exe 4 [gfxrecon] WARNING - This can lead to diverging driver behavior between the replayer and captured application 5 [gfxrecon] WARNING - Recommendation: Rename gfxrecon-replay.</description>
    </item>
    
    <item>
      <title>Intel RDT简介</title>
      <link>https://zhiwei-lii.github.io/intel-rdt%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Sun, 13 Jun 2021 13:50:36 +0800</pubDate>
      
      <guid>https://zhiwei-lii.github.io/intel-rdt%E7%AE%80%E4%BB%8B/</guid>
      <description>RDT要解决的就是: cache level的资源隔离. 我们先看一下noisy neighbour问题.
Noisy Neighbor问题 在目前Server的硬件架构中, 遵循以下规则:
 每个logical core独享L1 cache (L1又被进一步地分为icache, dcache) 每个physical core(对应两个logical core)共享L2 cache 每个socket的core共享L3 cache  这就有可能引发noisy neighbor问题. 如下图, 分别有两个application, 由于L3是共享的, 同时遵循&amp;quot;先到先得&amp;quot;的原则, App[1]只有少量的L3 cache可以利用, 这对于它的性能无疑是不利的. RDT分为5个功能模块： Cache Monitoring Technology (CMT) 缓存检测技术 Cache Allocation Technology (CAT) 缓存分配技术 Memory Bandwidth Monitoring (MBM) 内存带宽监测 Memory Bandwidth Allocation (MBA) 内存带宽分配 Code and Data Prioritization (CDP) 代码和数据优先级
下面主要介绍一下CAT和MBA的机制.
Cache Allocation Technology (CAT) RDT中定义了clos的概念, 可以将它类比为cgroup中的group. 一个clos对应了一种hareware config(比如20% L3 cache). 通过将上层应用映射到这些clos, 来达到资源控制的效果.</description>
    </item>
    
    <item>
      <title>Swiftshader简介</title>
      <link>https://zhiwei-lii.github.io/swiftshader%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Sun, 30 Aug 2020 13:50:36 +0800</pubDate>
      
      <guid>https://zhiwei-lii.github.io/swiftshader%E7%AE%80%E4%BB%8B/</guid>
      <description>Swiftshader是Google推出的OpenGL的软件实现. 它的机制和VM差不多, 可以动态地将shader翻译成cpu指令.
下图是它的架构, 主要有三层:
 Renderer: 负责将Shader中的操作转化为Reactor的调用. Reactor: 一种中间语言，主要就是包装了一下LLVM的调用. LLVM: Swiftshader底层通过LLVM来生成machine code.   本文分为三个部分:
 Reactor Layer Introduction Renderer Layer Introduction Debug Tips  Reactor Layer Introduction Reactor是一种中间语言, 可以嵌入在C++中使用. 它的语法参见Reactor.md
Reactor里定义了很多数据类型:
// src/Reactor/Reactor.hpp class Bool; ... class Short8; class UShort8; ... class Int4; class UInt4; ... class Float4; 可以使用RR_WATCH(variable_name)的形式来print Reactor变量. 前提是在CMakeList.txt中定义ENABLE_RR_PRINT
// src/Reactor/Print.hpp // RR_WATCH() is a helper that prints the name and value of all the supplied // arguments.</description>
    </item>
    
    <item>
      <title>Android PGO Guide</title>
      <link>https://zhiwei-lii.github.io/android-pgo-guide/</link>
      <pubDate>Fri, 10 Apr 2020 13:50:36 +0800</pubDate>
      
      <guid>https://zhiwei-lii.github.io/android-pgo-guide/</guid>
      <description>Part 1 采集数据  在fio的Android.bp加入下面的code.  pgo: { instrumentation: true, benchmarks: [ &amp;quot;fio&amp;quot;, // benchmarks可以理解为当前优化的workload的名字. ], profile_file: &amp;quot;fio.profdata&amp;quot;, // fio.profdata表示会去toolchain/pgo-profiles下面寻找该文件. } make fio ANDROID_PGO_INSTRUMENT=fio 运行binary/library, 它会在/data/local/tmp下面生成profraw的文件  # build/soong/cc/pgo.go const profileInstrumentFlag = &amp;quot;-fprofile-generate=/data/local/tmp&amp;quot; llvm-profdata merge -output=fio.profdata default_xxxxxx.profraw 注意llvm-profdata需要用Android prebuild的版本, 位于./prebuilts/clang/host/ llvm-profdata show -all-functions fio.profdata // 可以dump profile信息, 可以通过function id将default_xxx.profraw和具体的library/binary对应起来.  Part 2 使用profile来优化  把profdata放到toolchain/pgo-profiles make fio // 不设置ANDROID_PGO_INSTRUMENT的话, 它就不会插入profile的代码. run  PS:  进程会在exit的时候dump profile, 收到signal 9的时候并不会dump. 所以如果是要优化长期存在的service, 需要做些hack.  # external/compiler-rt/lib/profile/InstrProfilingFile.</description>
    </item>
    
    <item>
      <title>Android Art里的JIT&amp;AOT</title>
      <link>https://zhiwei-lii.github.io/android-art%E9%87%8C%E7%9A%84jitaot/</link>
      <pubDate>Mon, 10 Feb 2020 13:50:36 +0800</pubDate>
      
      <guid>https://zhiwei-lii.github.io/android-art%E9%87%8C%E7%9A%84jitaot/</guid>
      <description>简单介绍一下Art里的jit和aot. 本文分成三个部分:
 JIT Introduction AOT Introduction Relation between JIT&amp;amp;AOT  JIT Introduction JIT这里没有看太多的内容，先占个位:D
####Profile文件 Art会在app执行过程中对它做profiling, 记录hot method等优化信息, profiling的结果会存在storage当中, 以便下次执行的时候供art参考.
profile的文件格式如下:
324 /** 325 * Serialization format: 326 * [profile_header, zipped[[profile_line_header1, profile_line_header2...],[profile_line_data1, 327 * profile_line_data2...]]] 328 * profile_header: 329 * magic,version,number_of_dex_files,uncompressed_size_of_zipped_data,compressed_data_size 330 * profile_line_header: 331 * dex_location,number_of_classes,methods_region_size,dex_location_checksum,num_method_ids 332 * profile_line_data: 333 * method_encoding_1,method_encoding_2...,class_id1,class_id2...,startup/post startup bitmap 334 * The method_encoding is: 335 * method_id,number_of_inline_caches,inline_cache1,inline_cache2... 336 * The inline_cache is: 337 * dex_pc,[M|dex_map_size], dex_profile_index,class_id1,class_id2.</description>
    </item>
    
    <item>
      <title>Android处理Move事件时为何要等待vsync</title>
      <link>https://zhiwei-lii.github.io/android%E5%A4%84%E7%90%86move%E4%BA%8B%E4%BB%B6%E6%97%B6%E4%B8%BA%E4%BD%95%E8%A6%81%E7%AD%89%E5%BE%85vsync/</link>
      <pubDate>Mon, 10 Feb 2020 13:50:36 +0800</pubDate>
      
      <guid>https://zhiwei-lii.github.io/android%E5%A4%84%E7%90%86move%E4%BA%8B%E4%BB%B6%E6%97%B6%E4%B8%BA%E4%BD%95%E8%A6%81%E7%AD%89%E5%BE%85vsync/</guid>
      <description>根据https://www.jianshu.com/p/c2e26c6d4ac1 Android处理down的时候是直接处理的, 但是处理move的时候需要等待vsync. 但是为什么要等待vsync呢? 这个等待时间能不能去掉, 以优化latency? 我的理解是: 不能. Android这么设计是有原因的.
前提假设:
1. 一个batch中的move间隔很短. (&amp;lt;16ms) 2. 下一帧的画面更新只能根据一个move事件来update. (因为假设app在一帧以内收到了好几个move事件, 它不可能在下一帧同时响应这几个move事件. 一次画面更新只能对应一个事件) 所以这两个前提推出一个结论就是:
1. 一个batch中的move事件需要进行合并, 合并成一个或两个Motionevent. 那么以什么标准, 怎么样去合并move呢? 我只能想到以vsync来作为标准. 因为很可能会出现下面这种情况. 一个batch的move跨越了一个vsync. 所以这个batch的move应该根据vsync时间来划分成两个event. 所以move的合并过程会依赖于vsync的时间戳. 所以只能等vsync到了, 才能做这个事情.
相关代码: (合并多个move为一个MotionEvent)
661 status_t InputConsumer::consumeSamples(InputEventFactoryInterface* factory, 662 Batch&amp;amp; batch, size_t count, uint32_t* outSeq, InputEvent** outEvent, int32_t* displayId) { 663 MotionEvent* motionEvent = factory-&amp;gt;createMotionEvent(); 664 if (! motionEvent) return NO_MEMORY; 665 666 uint32_t chain = 0; 667 for (size_t i = 0; i &amp;lt; count; i++) { // count指的是batch中第一个晚于vsync的move的index 668 InputMessage&amp;amp; msg = batch.</description>
    </item>
    
  </channel>
</rss>
